# Import required libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix

# Step 1: Load the dataset
df = pd.read_csv('iris.csv')  # Load Iris dataset from a CSV file

# Step 2: Split the dataset into features (X) and target (y)
X = df.iloc[:, :-1]  # Select all columns except the last as features
y = df.iloc[:, -1]   # Select the last column as target (class labels)

# Step 3: Split the data into training and test sets (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Step 4: Create and train the Gaussian Na√Øve Bayes classifier
model = GaussianNB()            # Initialize the model
model.fit(X_train, y_train)     # Train the model on training data

# Step 5: Predict class labels for test data
y_pred = model.predict(X_test)  # Use the trained model to predict test data

# Step 6: Create a confusion matrix to compare actual vs predicted values
cm = confusion_matrix(y_test, y_pred, labels=y.unique())
print("Confusion Matrix:\n", cm)

# Step 7: Calculate performance metrics for each class
total = cm.sum()        # Total number of predictions
labels = y.unique()     # Get the unique class labels

# Loop through each class to compute metrics
for i, label in enumerate(labels):
    TP = cm[i, i]                        # True Positives for this class
    FP = cm[:, i].sum() - TP            # False Positives: predicted as this class incorrectly
    FN = cm[i, :].sum() - TP            # False Negatives: actually this class but predicted wrong
    TN = total - TP - FP - FN           # True Negatives: all other correct predictions

    # Calculate metrics
    acc = (TP + TN) / total             # Accuracy: how many predictions were correct
    prec = TP / (TP + FP) if TP + FP else 0  # Precision: how many predicted positives were correct
    rec = TP / (TP + FN) if TP + FN else 0   # Recall: how many actual positives were correctly predicted
    err = 1 - acc                        # Error rate: 1 - accuracy

    # Print results for this class
    print(f"\nClass: {label}")
    print(f"TP={TP}, FP={FP}, TN={TN}, FN={FN}")
    print(f"Accuracy: {acc:.2f}, Error Rate: {err:.2f}")
    print(f"Precision: {prec:.2f}, Recall: {rec:.2f}")
