{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8076f91-f31b-44c2-940b-33f5488009dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document 1 ---\n",
      "Tokens: ['natural', 'language', 'processing', 'is', 'a', 'fascinating', 'field', 'of', 'ai', '.']\n",
      "POS Tags: [('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('fascinating', 'JJ'), ('field', 'NN'), ('of', 'IN'), ('ai', 'NN'), ('.', '.')]\n",
      "Filtered: ['natural', 'language', 'processing', 'fascinating', 'field', 'ai']\n",
      "Stemmed: ['natur', 'languag', 'process', 'fascin', 'field', 'ai']\n",
      "Lemmatized: ['natural', 'language', 'processing', 'fascinating', 'field', 'ai']\n",
      "\n",
      "--- Document 2 ---\n",
      "Tokens: ['text', 'analytics', 'involves', 'preprocessing', 'and', 'transforming', 'text', 'data', '.']\n",
      "POS Tags: [('text', 'JJ'), ('analytics', 'NNS'), ('involves', 'VBZ'), ('preprocessing', 'VBG'), ('and', 'CC'), ('transforming', 'VBG'), ('text', 'NN'), ('data', 'NNS'), ('.', '.')]\n",
      "Filtered: ['text', 'analytics', 'involves', 'preprocessing', 'transforming', 'text', 'data']\n",
      "Stemmed: ['text', 'analyt', 'involv', 'preprocess', 'transform', 'text', 'data']\n",
      "Lemmatized: ['text', 'analytics', 'involve', 'preprocessing', 'transform', 'text', 'data']\n",
      "\n",
      "--- Document 3 ---\n",
      "Tokens: ['lemmatization', 'returns', 'dictionary', 'forms', 'of', 'words', ',', 'unlike', 'stemming', '.']\n",
      "POS Tags: [('lemmatization', 'NN'), ('returns', 'NNS'), ('dictionary', 'JJ'), ('forms', 'NNS'), ('of', 'IN'), ('words', 'NNS'), (',', ','), ('unlike', 'IN'), ('stemming', 'VBG'), ('.', '.')]\n",
      "Filtered: ['lemmatization', 'returns', 'dictionary', 'forms', 'words', 'unlike', 'stemming']\n",
      "Stemmed: ['lemmat', 'return', 'dictionari', 'form', 'word', 'unlik', 'stem']\n",
      "Lemmatized: ['lemmatization', 'return', 'dictionary', 'form', 'word', 'unlike', 'stem']\n",
      "\n",
      "TF-IDF Representation:\n",
      "\n",
      "         ai  analytics      data  dictionary  fascinating     field     forms  \\\n",
      "0  0.408248   0.000000  0.000000    0.000000     0.408248  0.408248  0.000000   \n",
      "1  0.000000   0.333333  0.333333    0.000000     0.000000  0.000000  0.000000   \n",
      "2  0.000000   0.000000  0.000000    0.377964     0.000000  0.000000  0.377964   \n",
      "\n",
      "   involves  language  lemmatization   natural  preprocessing  processing  \\\n",
      "0  0.000000  0.408248       0.000000  0.408248       0.000000    0.408248   \n",
      "1  0.333333  0.000000       0.000000  0.000000       0.333333    0.000000   \n",
      "2  0.000000  0.000000       0.377964  0.000000       0.000000    0.000000   \n",
      "\n",
      "    returns  stemming      text  transforming    unlike     words  \n",
      "0  0.000000  0.000000  0.000000      0.000000  0.000000  0.000000  \n",
      "1  0.000000  0.000000  0.666667      0.333333  0.000000  0.000000  \n",
      "2  0.377964  0.377964  0.000000      0.000000  0.377964  0.377964  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Jayesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jayesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jayesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jayesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jayesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import required libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2: Download necessary NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Step 3: Sample documents\n",
    "documents = [\n",
    "    \"Natural Language Processing is a fascinating field of AI.\",\n",
    "    \"Text analytics involves preprocessing and transforming text data.\",\n",
    "    \"Lemmatization returns dictionary forms of words, unlike stemming.\"\n",
    "]\n",
    "\n",
    "# Step 4: Helper function to map POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Step 5: Preprocessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(doc):\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    filtered = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
    "    stemmed = [ps.stem(w) for w in filtered]\n",
    "    lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(pos)) for w, pos in pos_tags if w.isalpha() and w not in stop_words]\n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"pos_tags\": pos_tags,\n",
    "        \"filtered\": filtered,\n",
    "        \"stemmed\": stemmed,\n",
    "        \"lemmatized\": lemmatized\n",
    "    }\n",
    "\n",
    "# Step 6: Apply preprocessing to each document\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    result = preprocess(doc)\n",
    "    print(\"Tokens:\", result[\"tokens\"])\n",
    "    print(\"POS Tags:\", result[\"pos_tags\"])\n",
    "    print(\"Filtered:\", result[\"filtered\"])\n",
    "    print(\"Stemmed:\", result[\"stemmed\"])\n",
    "    print(\"Lemmatized:\", result[\"lemmatized\"])\n",
    "\n",
    "# Step 7: TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Step 8: Display TF-IDF matrix\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"\\nTF-IDF Representation:\\n\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4691ab-696d-4fa3-8dc9-1e6f27b60de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
